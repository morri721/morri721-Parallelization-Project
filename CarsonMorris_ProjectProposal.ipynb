{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4710c750",
   "metadata": {},
   "source": [
    "# Parallelization of MRI Image Segmentation Model via TensorFlow\n",
    "\n",
    "By Carson Morris\n",
    "\n",
    "&#9989; Replace the following with a picture that \"defines\" your project.  This could be software logo, an expected outcome of your project, or a graphical representation of the research area. \n",
    "\n",
    "<img alt=\"Simple Icon of a camera. This is just a place holder for your image\" src=\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12902-022-01107-2/MediaObjects/12902_2022_1107_Fig1_HTML.png?as=webp\" width=\"40%\">\n",
    "\n",
    "Image from: [http://simpleicon.com/](http://simpleicon.com/)\n",
    "\n",
    "---\n",
    "### Abstract\n",
    "\n",
    "For this project, I plan to explore parallelization in Keras, which is a high-level neural network Python library that runs on top of TensorFlow. This will be done in collaboration with my CMSE495 capstone project, which focuses on creating a neural network to properly segment MRI images of the deltoid muscle. Training a neural network on a large amount of images will be very computationally expensive, so being able to speed-up the training process will be a big plus for myself and my group. One neural network that we'll most-likely use as a starting point is a U-Net, since it's very good for solving this type of problem. I will benchmark the time it takes to train the model on the set of MRI images (we're unsure as to how big the dataset will be at this point), and from there try to decrease the average training time after incorporating parallelization. A successful outcome will not only be being able to achieve a significant performance increase (by lowering the average training time), but also getting more experience training models with TensorFlow. \n",
    "\n",
    "\n",
    "----\n",
    "### Schedule\n",
    "\n",
    "* Sunday February 5 - Project Proposal Milestone Due\n",
    "* Week of February - Modify proposal if necessary, start researching parallelization methods in TensorFlow (MultiWorker stratigies, and how to use GPUs).\n",
    "* Week of February - Benchmark baseline model. Start incorporating parallelization MultiWorker strategy.\n",
    "* Week of February - Finalize implementation (work out any issues).Have the model running in parallel with one strategy, and hopefully one GPU.\n",
    "* Sunday February 26 - Project Part 1 Due\n",
    "* Week of March - Research the other MultiWorker strategies, pick a few to compare to the baseline strategy, and baseline (unparallel) model.\n",
    "* Week of March - Test another strategy. Compare results.\n",
    "* Week of March - Test another strategy. Compare results.\n",
    "* Week of March - Test another strategy (maybe). Choose the \"best\" strategy. \n",
    "* Week of April - Test with more powerful hardware on the HPCC (utilize TensorFlows GPU processing capabilities). \n",
    "* Week of April - Finalize results.\n",
    "* April 15 - Final Project due\n",
    "\n",
    "---\n",
    "### Part 1 Software Exploration\n",
    "\n",
    "The main focus of this project will be incorporating parallelization into TensorFlow. There are many resources online on how to do this, but a good starting point will be from the Keras documentation itself: (https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras). Based on this tutorial, a large portion of this project will be determining which \"MultiWorker strategy\" should be used. This will affect how the training workload is distributed, so it will be important to test out several to see which gives the highest performance increase. One feature of TensorFlow is being able to utilize GPU's, so I will explore how to take advantage of this on the HPCC.\n",
    "\n",
    "One challenge that I've already encountered when exploring this idea was properly installing all of the packages. TensorFlow is a huge library with many built-in Ideally, I will include a 'makefile' on github which will properly setup the environment and install necessary packages. Overall, my goal for this section is to learn more about the advantages of the various TF MultiWorker strategies, and successfully implement one of these methods into our existing neural network.\n",
    "\n",
    "---\n",
    "### Part 2 Benchmark and Optimization\n",
    "\n",
    "As previously mentioned, TensorFlow has built-in methods for utilizing parallelization, as well as performance tracking which can be used to monitor the throughput of the model (https://www.tensorflow.org/api_docs/python/tf/test/Benchmark). After I'm able to succesfully implement one of these methods and get a baseline metric, I will then test out the other MultiWorker strategies to see which one offers the greatest performance increase. Once the ideal strategy has been chosen, I will see if the performance can increase further by increasing the computing power of the HPCC by adding GPU's (https://www.tensorflow.org/guide/gpu).\n",
    "\n",
    "In order to consistently track performance, it's important to use the same sized training set each time. I will use Python's built-in time module to track the time it takes to train from start to finish, and then save these times in a seperate file. Because training times can vary, I will run the model at least 3 times (maybe more depending on the variability) in order to obtain an average. This will most likely be very time consuming (especially when getting a baseline time, before implementing parallelization) so I will be sure to start this process as soon as possible. \n",
    "\n",
    "While it's hard to give an exact number on the expected performance increase since we have yet to start training this model in my Capstone group, I hope to demonstrate that parallelization will indeed speed up the time it takes to train a neural network on image data. My goal is to demonstrate that the traing time can be reduced significantly when using high-dimensional datasets, such as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91980c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
